<!DOCTYPE HTML>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>AutoEncoder - Trevor C.</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Header -->
			<header id="header">
				<a href="index.html" class="title">Trevor's Projects</a>
				<nav>
					<ul>
						<li><a href="index.html">Home</a></li>
						<li><a href="index.html#Projects">Projects</a></li>
						<!--<li><a href="index.html#WorkExperience">Work Experience</a></li>-->
						<!--<li><a href="index.html#Hobbies">Hobbies</a></li>-->
						<li><a href="index.html#Contact">Contact</a></li>
					</ul>
				</nav>
			</header>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<section id="main" class="wrapper">
						<div class="inner">
							<h1 class="major">Auto-Encoder for Anomaly Detection</h1>
							<p>This page is less technical than the <a href="https://github.com/jtclark2/AutoEncoderAndBinaryClassifier/blob/main/AutoEncoder%20And%20AnomalyDetector.ipynb" >Jupyter notebook</a>, but still assumes familiarity with CNNs.</p>

							<h2><b>Project Goals and Motivation: Expanding our tool belt</b></h2>
							<!--Consider adding padding to this image...it's simple and useful: https://clemson.instructure.com/courses/12011/pages/image-padding-->
							<!--<img src="images/AutoEncoder/ToAHammerEveryProblemIsANail.jpg" alt="Which one doesn't belong?" width="25%" align="left" margin="5%">-->
							<p>This project explores tools (classifiers and auto-encoders) as alternatives to traditional
								computer vision tools. In particular, I will be detecting the presence of welds on a part.
								This is not necessarily a better approach for this application, but it adds tremondous
								power in the right circumstances. For our purposes, it will make a good case-study as I
								describe the technology and some of the advantages/disadvantages of this technique.
								I'll focus far more on the auto-encoder, simply because classifiers are so prevalent.
							</p>


							<p>For this application, I wanted to answer a few questions:
							<ul>
								<li>Could a program be created that didn't need any specific knowledge of the features
									being detecting? If so, the program could generalize to new applications, with
									changes isolated to training data and hyperparameters.
								</li>
								<li>Could a model be trained with a small dataset (~150 images). Deep learning has a reputation
								of requiring large amounts of data, but given how well controlled these images are, it
									seemed plausible.</li>
								<li>Can problems/anamolies be detected, even if they were not explicitly anticipated during training?</li>
							</ul>
							</p>



							<h2>Simple Detection / Binary Classification</h2>
							<p>
								Binary Classification is the obvious choice for determining the presence of a specific
								feature/defect, since it is effectively a binary classification problem. I wrote it for
								a slightly different shaped data-set, but for confidentiality reasons (the dataset
								was based on an unreleased product from a previous job), I won't share those images
								here.
								Rather than re-wrinting it for the weld data I'll be sharing below, I created another data set
								with pictures of a stuffed Totoro (from the beloved Miyazaki movie). This also has the advantage
								of showing a much more varied / difficult dataset.
							</p>
							<p>
								The program makes good use of Tensorflow tools that greatly simplify data loading and
								augmentation. If I were to repeat this project, I would use transfer learning with
								an established model, such as VGG-16, and just freeze the last few layers. I did
								something similar in <a href="https://github.com/jtclark2/ImageCaptioning/blob/main/model.py">this project.</a></p>
								Feel free to look through <a href="https://github.com/jtclark2/AutoEncoderAndBinaryClassifier/blob/main/BinaryClassifier.ipynb">that notebook</a>
								for details, though we'll spend the rest of this page discussing the auto-encoder application.
							</p>
								<img src="images/AutoEncoder/TotoroClassifier.JPG">

							<h2>What is an Auto-encoder?</h2>
							<p>An auto-encoder is a FeedForward CNN (Convolutional Neural Network) that compresses an
								image into a small hidden layer, sometimes called a latent space, and then reverses that
								operation to restore the original image. The act of encoding the image data into a lower
								dimensional space is what lends the model its name.</p>
							<p>
								<b>Compression and Specialization:</b> This latent space is a compressed version of that image. This
								compression can be much more efficient than traditional methods, though it is highly
								specialized to the type of data it has trained on.
								Imagine a highly trained musician looking at a piece of music. In a glance, they can
								learn a lot about that piece, remembering the music much more effectively than a person
								without that training. A doctor might similarly understand a brain-scan with a glance. However,
								the highly trained musician does not have any skill to help them memorize brain scans.
								Both are highly trained, and highly specialized.
							</p>
							<p><b>Lossy images</b> High compression ratio is an excellent benefit; however, it comes at
								the price of being a lossy compression model. That means that even a very good model will
								not remember/recreate the original image perfectly. From our previous example, the musician
								who glanced at a piece of music might be able to play parts of the piece well, but they might
								play chords in a slightly different way or improvize parts. Futhermore, when that musician
								tries to recreate the brain scan, they will probably miss most of the important information.
								This means that when an image is restored, it will be able to approximate
								the original image, rebuilding it with the features we've observed, but failing how
								understanding how to reconstruct anything else. While this may sound like a limitation,
								it's actually the first step to distinguishing different types of information in our image.
							</p>
						Lossy Compression (using <b>well</b> specialized features)
								<br>
								<img src="images/AutoEncoder/LossyCompression.JPG", alt="Lossy Compression Example">
								<br>
						Loss Compression (using <b>poorly</b> specialized feature, which almost completely drops the weld feature)
								<br>
								<img src="images/AutoEncoder/PoorlySpecializedCompression.JPG" alt="PoorlySpecializedCompression.JPG"/>
							<p>
								<b>Denoising:</b> Using the fact that our models are lossy and specialized, we can start
								doing something really interesting. We can start remembering only the things that matter
								to us. One common application of this is to train a model on very clean images,
								so that it starts to learning a set of underlying patterns/symbols. After that is trained,
								feed it noisy data. Because it has learned to understand the signal, but not the noise,
								it saves the clean data, and doesn't have the ability to remember the specifics of the
								random noise. When this image is decompressed, the noise has disappeared, and the image
								has been denoised.
								<a href="https://keras.io/examples/vision/autoencoder/">Keras has a simple example that shows how powerful this can be, even with VERY noisy images.</a>
								<img src="images/AutoEncoder/autoencoder_noisy_digits.png" alt="" />

							</p>
							<p><b>Anomaly Detection:</b> Similarly, I can train a model to remember a nominal image,
								representing all the features we hope to see. This model learns to reproduce only the
								features we've taught it about. Now, if new feature appears in an image, our latent
								space in unable to save it. As a result, we regenerate it incorrectly, producing junk or
								trying to create the object using the features that we trained on. Finally, we just
								compare this new image to the original, and we can isolate any poorly reproduced portions,
								building a heat map of foreign features in a scene.
							<img src="images/AutoEncoder/AnomalyDetected.JPG" alt="" data-position="50% 25%" />
							</p>

							<p>As you can see, the output of the loss function is a heat map of the anomalous parts of
								the image. Since this model trained on unwelded data, welds are showing up as anomalies.
								We've effectively generated a heat-map of how much each image looks like a weld. If I was
								still working with my old welding team, I would love to learn more about what makes a
								"good" weld, so we could create metrics to quantify weld "quality".
							</p>
							<p><b>Theoretical side-note:</b> This output looks very similar to the output
								of a segmentation problem. However, there are a few notable differences. In true
								segmentation, the output would be an image of labeled classes. In contrast, we've applied
								the loss function to generate a real-valued image. However, in both cases, we have effectively
								highlighted the weld region.
								<p> A key difference is that the segmentation problem requires labeled data, whereas
								this technique does not. Technically, we just re-use the input image as the label, though
								nothing needs to be generated externally. This is one of the few applications I'm aware
								of in which deep learning is used as an unsupervised model. It requires a bit of
								post-processing, but in return, we gain a way to find ANY outliers, not just data that
								vary in the direction of a pre-labeled classes.</p>
							<p>

							</p>

							<h2>The processing pipeline:</h2>
							<ol>
								<li><b>Pre-Processing:</b></li>
									<ol> <!--TODO: Consider a, b, c numbering: https://www.w3schools.com/tags/att_ol_type.asp -->
										<li>Load images (false color heat maps are used instead of grayscale)</li>
											<img src="images/AutoEncoder/RawNominal.JPG" alt="Show a raw image (nominal and anomalous)">
											<img src="images/AutoEncoder/RawAnomalous.JPG" alt="Show a raw image (nominal and anomalous)">
										<li>resize images</li>
										<li>Convert BGR to RGB</li>
										<li>Equalize Histograms: This technique spreads the histrogram out evenly, such
											that the Cumulative Distribution Function (CDF) becomes linear. In practice,
											this spreads out images that have many pixels clumped in bright/dark patches
											allowing for much better separation in intensity.
										</li>
										<img src="images/AutoEncoder/HistogramEqualization.JPG" alt="Show the image equalized (nominal and anomalous)">
										<li>Normalize values</li>
										<li>Convert to numpy arrays</li>
										<li>Crop (if requested - this is an optional parameter that is easy to toggle on/off)</li>
									</ol>
								<li>
									<p>
										<b>Model</b>: This model uses a feed-forward CNN to reduce the dimensionality
										to the latent space, and then the reverse to scale it back up to an image.
										Compression is repeated 4 times. Each compression consists of a 2D convolution
										followed by a max-pooling layer. The number of channels is gradually increased as the
										spatial dimensions are quickly decreased. The decompression is just the inverse.
										It is performed in 4 steps as well, using Convolution 2D Transpose layers, increasing
										the dimensionality back to the original (256,256,1) dimensions of the image.
										The exact parameters of this compression were hand tuned experimentally in order
										to find a compression layer that was good at capturing trained data, but bad at
										generalizing to new data. In other words, high specialization, and low-moderate loss.
									</p>
									<p>I found that increasing the channel dimension was critical to getting good results,
										since I'm looking for local features. This isn't the case in the simple denoiser, where
										you actually want to hold onto much of the spatial information. Note that the
										output images are blurred. This is a limitation of the current model. It can
										be mitigated in 2 ways. First, we could use a larger/more complex model, though
										the focus of this application is discriminating feature, not producing a perfect
										heat-map of differences. Second we could introduce skip-nets, which allow CNNs to bypass certain layers, making it
										easier restore full resolution images. However, this is beyond the current scope.
									</p>
									<p>Note that the image on the left is a fairly good representation of our input, while
										the model on the right now seems to be missing the anomalous weld feature. This
										means we have succeeded.</p>
									<img src="images/AutoEncoder/PredictedNominal.JPG" alt="Model Output (nominal and anomalous)">
									<img src="images/AutoEncoder/PredictedAnomalous.JPG" alt="Model Output (nominal and anomalous)">
								</li>
								<li><b>Post-Processing:</b></li>
								<ol>
									<li>Use loss function calculate the differences between the original and predicted images.
									At this stage, the image is a great heat-map, and can be useful for manual inspection.</li>
									<img src="images/AutoEncoder/LossNominal.JPG" alt="Loss Image (nominal and anomalous)">
									<img src="images/AutoEncoder/LossAnomalous.JPG" alt="Loss Image (nominal and anomalous)">
									<li>Binarize the anomaly image, based in a tuned threshold</li>
									<img src="images/AutoEncoder/BinaryNominal.JPG" alt="Binary Image (nominal and anomalous)">
									<img src="images/AutoEncoder/BinaryAnomalous.JPG" alt="Binary Image (nominal and anomalous)">
									<li>Denoise: I wrote a custom algorithm based on connected clusters, which denoises this image pretty well.</li>
									<img src="images/AutoEncoder/DenoisedNominal.JPG" alt="Denoised Image (nominal and anomalous)">
									<img src="images/AutoEncoder/DenoisedAnomalous.JPG" alt="Denoised Image (nominal and anomalous)">
									<li>(optional) Crop images to a region of interest: This is not required, but can
									be useful to strengthen signal to noise ratio for the detection of a specific feature.</li>
									<li>Take the mean of the image, as a summary metric to represent how anamalous the image was.</li>
								</ol>
								<li><b>Analysis:</b> See notebook for a discussion of different analysis tools that can
									be used to set the threshold between these nominal/anomalous categories, and the
									trade-offs that different methods are subject to. The usual machine learning tools
									are bit different from the manufacturing process control tools, since ML
									researchers and manufacturers have different concerns. This includes some
									light discussion of Cpk and plotting precision-recall curves and false-alarm/escape curves.</li>
									<img src="images/AutoEncoder/PrecisionRecall_FalseAlarm_Escape_Curves.JPG" alt="Result Distribution">
									<img src="images/AutoEncoder/LossScoresAcrossAllImages.JPG" alt="Resuling CurveCurves">
							</ol>

							<a href="https://github.com/jtclark2/AutoEncoderAndBinaryClassifier" class="icon brands fa-github"><b>
									View full source code on github</b><span class="label">GitHub</span></a>
						</div>
					</section>

			</div>

		<!-- Footer -->
			<footer id="footer" class="wrapper alt">
				<div class="inner">
					<ul class="menu">
						<li>&copy; Untitled. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
					</ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>